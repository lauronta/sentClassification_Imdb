{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche deep learning - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset import load_data_film\n",
    "\n",
    "\n",
    "DATASET_PATH_TRAIN = Path(\"data/aclImdb/train\")\n",
    "DATASET_PATH_TEST = Path(\"data/aclImdb/test\")\n",
    "\n",
    "NB_DOC_MAX = 1000 # par classe\n",
    "IMDB_CLASSES  = ['neg','pos']\n",
    "VOC_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "txts, files, filelabels = load_data_film(DATASET_PATH_TRAIN, max_size = NB_DOC_MAX)\n",
    "txts_test, files_test, filelabels_test = load_data_film(DATASET_PATH_TEST, max_size = NB_DOC_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser le modèle suivant, il s'agot d'un modèle bert, finetuné pour de la classification de sentiment\n",
    "https://huggingface.co/tabularisai/multilingual-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche on the shelf - Modèle HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"tabularisai/multilingual-sentiment-analysis\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def predict_sentiment(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, I know that I voted 1 out of 10 but really this deserves no more than half of a star. I hated it. It was so stupid and unrealistic, I can't believe any of the stars signed on to make this ridiculously absurd project.<br /><br />James G. and Cathrine O'Hara were excellent in their characters and Ben Affleck and Christina Applegate were just as good too, but the story sucked and I encourage anyone who sees this in the video store to not even bother picking it up and reading the back cover, but to just walk away...I don't even want to get into what the movie is about, because it is too stupid to pontificate about.<br /><br />Don't rent this! It's horrible! Horrible!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Very Negative']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(txts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approche custom - Modèle maison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On construit un tokenizer à l'aides de la librairie tokenizers d'HuggingFace. On utilise sensiblement les même paramètres que pour le tokenizer de Bert en changeant la taille de vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.isfile('bert-wiki.json'):\n",
    "    tokenizer = Tokenizer.from_file(\"bert-wiki.json\")\n",
    "else :  \n",
    "\n",
    "    from tokenizers.models import WordPiece\n",
    "    from tokenizers import normalizers\n",
    "    from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "    from tokenizers.pre_tokenizers import Whitespace\n",
    "    from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=VOC_SIZE, \n",
    "        special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    "        )\n",
    "\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    tokenizer.train(files, trainer)\n",
    "    tokenizer.save(\"bert-wiki.json\")\n",
    "\n",
    "PAD = tokenizer.encode('[CLS]').ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: list, labels):\n",
    "        self.labels = labels\n",
    "        self.phrasesnum = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.phrasesnum[i], torch.tensor(self.labels[i])\n",
    "\n",
    "## create  datasets\n",
    "ds_train = TextDataset(txts,filelabels)\n",
    "ds_test  = TextDataset(txts_test,filelabels_test)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences  = [torch.tensor(tokenizer.encode('[CLS] ' + seq).ids) for seq in sequences]\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=False)\n",
    "    return padded_sequences, torch.tensor(lengths), torch.tensor(labels)\n",
    "\n",
    "\n",
    "## create dataloader\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn)\n",
    "test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded sequences: tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [ 235, 2201,   45,  ...,  250,  209,  250],\n",
      "        [ 517,  776, 7095,  ...,  276, 4860,  233],\n",
      "        ...,\n",
      "        [5126,    0,    0,  ...,    0,    0,    0],\n",
      "        [  20,    0,    0,  ...,    0,    0,    0],\n",
      "        [ 209,    0,    0,  ...,    0,    0,    0]])\n",
      "torch.Size([249, 8]) tensor(249)\n",
      "Labels: tensor([1, 1, 1, 1, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch =next(iter( train_loader))\n",
    "padded_sequences, lengths, labels = batch\n",
    "\n",
    "print(\"Padded sequences:\", padded_sequences)\n",
    "\n",
    "print(padded_sequences.size(), lengths.max())\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see the file models.py for model definition\n",
    "\n",
    "import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lauronta/Bureau/IODAA/iodaa/SDA_APA/tuto_deep/notebooks/.conda/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "## Network creation\n",
    "emb_size = 128\n",
    "voc_size = VOC_SIZE\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "hidden_size_mlp = 128\n",
    "output_size = 2\n",
    "\n",
    "# build network\n",
    "sent_clf = models.SentModel( emb_size, voc_size, num_layers, num_heads, hidden_size_mlp , output_size, PAD)\n",
    "sent_clf.name = \"SentModel-\"+time.asctime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_model(model,fichier): \n",
    "      \"\"\" sauvegarde du modèle dans fichier \"\"\"\n",
    "      state = {'model_state': model.state_dict()}\n",
    "      torch.save(state,fichier) \n",
    " \n",
    "def load_model(fichier,model):\n",
    "      \"\"\" Si le fichier existe, on charge le modèle  \"\"\"\n",
    "      if os.path.isfile(fichier):\n",
    "          state = torch.load(fichier)\n",
    "          model.load_state_dict(state['model_state'])\n",
    "      else:\n",
    "           print(\"Erreur de chargement du fichier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la métrique d'évaluation\n",
    "def accuracy(yhat,y):\n",
    "    # y encode les indexes, s'assurer de la bonne taille de tenseur\n",
    "    assert len(y.shape)==1 or y.size(1)==1\n",
    "    return (torch.argmax(yhat,1).view(y.size(0),-1)== y.view(-1,1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "device = \"cpu\"\n",
    "\n",
    "def train(model,epochs,train_loader,test_loader):\n",
    "    #writer = SummaryWriter(f\"{TB_PATH}/{model.name}\")\n",
    "    optim = torch.optim.Adam(model.parameters(),lr=5e-4)    # choix optimizer\n",
    "    model = model.to(device)\n",
    "    print(f\"running {model.name}\")\n",
    "    loss = nn.CrossEntropyLoss()                            # choix loss\n",
    "    # \n",
    "    # loss = nn.CrossEntropyLoss(weight=cl_weight.to(device))                            # choix loss\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        cumloss, cumacc, count = 0, 0, 0\n",
    "        model.train()\n",
    "        for x, lengths, y in tqdm(train_loader):                            # boucle sur les batchs\n",
    "            optim.zero_grad()\n",
    "            x,y = x.to(device), y.to(device)                # y doit être un tensor (pas un int)\n",
    "            yhat = model(x)\n",
    "            l = loss(yhat,y)\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            cumloss += l*len(x)                             # attention, il peut y avoir un batch + petit (le dernier)\n",
    "            cumacc += accuracy(yhat,y)*len(x)\n",
    "            count += len(x)\n",
    "        #writer.add_scalar('loss/train',cumloss/count,epoch)\n",
    "        #writer.add_scalar('accuracy/train',cumacc/count,epoch)\n",
    "        if epoch % 2 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                cumloss, cumacc, count = 0, 0, 0\n",
    "                for x, lengths, y in test_loader:\n",
    "                    x,y = x.to(device), y.to(device)\n",
    "                    yhat = model(x)\n",
    "                    cumloss += loss(yhat,y)*len(x)\n",
    "                    cumacc += accuracy(yhat,y)*len(x)\n",
    "                    count += len(x)\n",
    "                #writer.add_scalar(f'loss/test',cumloss/count,epoch)\n",
    "                #writer.add_scalar('accuracy/test',cumacc/count,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running SentModel-Sun Feb 16 22:11:59 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([259, 8, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([246, 8, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([249, 8, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255, 8, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/250 [00:11<15:46,  3.83s/it]\n",
      "  0%|          | 0/1 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     18\u001b[0m yhat \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     19\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(yhat,y)\n\u001b[0;32m---> 20\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m cumloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(x)                             \u001b[38;5;66;03m# attention, il peut y avoir un batch + petit (le dernier)\u001b[39;00m\n",
      "File \u001b[0;32m~/Bureau/IODAA/iodaa/SDA_APA/tuto_deep/notebooks/.conda/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bureau/IODAA/iodaa/SDA_APA/tuto_deep/notebooks/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Bureau/IODAA/iodaa/SDA_APA/tuto_deep/notebooks/.conda/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 1\n",
    "train(sent_clf, n_epoch, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 HyperParameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Study\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    return\n",
    "\n",
    "\n",
    "study = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
